{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build a encoder-decoder model order input from least to greastest.  first step just return the indices of fixed lenght input, second step variable lenght input, embedding layer and pointer network. \n",
    "\n",
    "Note that sequences are of fixed length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bm255022/virtualenvs/tensorflow27/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Global config variables\n",
    "num_steps = 4 \n",
    "batch_size = 16\n",
    "state_size = 12\n",
    "learning_rate = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def gen_data(time_steps, batch_size, num_batches):\n",
    "    X = np.random.rand(num_batches * batch_size, time_steps)\n",
    "    Y = np.asarray([np.argsort(e) for e in X])\n",
    "    for i in range(num_batches):\n",
    "        _x = X[i * batch_size:(i + 1) * batch_size]\n",
    "        _y = Y[i * batch_size:(i + 1) * batch_size]\n",
    "        yield _x, _y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [batch_size, num_steps], name='input_placeholder')\n",
    "y = tf.placeholder(tf.int32, [batch_size, num_steps], name='labels_placeholder')\n",
    "\n",
    "\n",
    "#add a dimension so matrix multiplication works later on.  we really want [batch, steps, features]\n",
    "rnn_inputs = tf.expand_dims(x, 2)\n",
    "# rnn_inputs is a list of num_steps tensors with shape [batch_size, num_classes]\n",
    "rnn_inputs = tf.unstack(rnn_inputs, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def rnn_cell(rnn_input, state, W, b):\n",
    "     return tf.tanh(tf.matmul(tf.concat([rnn_input, state], 1), W) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Tanh_3:0' shape=(16, 12) dtype=float32>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.variable_scope('rnn_cell'):\n",
    "    W = tf.get_variable('W', [1 + state_size, state_size])\n",
    "    b = tf.get_variable('b', [state_size], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "init_state = tf.zeros([batch_size, state_size])\n",
    "state = init_state\n",
    "rnn_outputs = []\n",
    "for rnn_input in rnn_inputs:\n",
    "    state = rnn_cell(rnn_input, state, W, b)\n",
    "    rnn_outputs.append(state)\n",
    "final_state = rnn_outputs[-1]\n",
    "final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#decoder inputs\n",
    "\n",
    "# rnn_inputs is a list of num_steps tensors with shape [batch_size, num_classes]\n",
    "decoder_inputs = tf.expand_dims(y, 2)\n",
    "decoder_inputs = tf.unstack(decoder_inputs, axis=1)\n",
    "\n",
    "#we want to insert starter symbols at beginning of list and drop last element of list\n",
    "start = tf.ones([batch_size, 1])\n",
    "start = tf.scalar_mul(-1, start)\n",
    "decoder_inputs.insert(0, start)\n",
    "del decoder_inputs[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#decoder cell\n",
    "with tf.variable_scope('decoder_cell'):\n",
    "    W = tf.get_variable('W', [1 + state_size, state_size])\n",
    "    b = tf.get_variable('b', [state_size], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "state = final_state #start with state from encoder rnn\n",
    "decoder_outputs = []\n",
    "for decoder_input in decoder_inputs:\n",
    "    #need to convert decoder_input to float in order to concat with state?\n",
    "    decoder_input = tf.cast(decoder_input, dtype=tf.float32)\n",
    "    state = rnn_cell(decoder_input, state, W, b)\n",
    "    decoder_outputs.append(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'stack:0' shape=(16, 4, 4) dtype=float32>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = num_steps\n",
    "with tf.variable_scope('softmax'):\n",
    "    W = tf.get_variable('W', [state_size, num_classes])\n",
    "    b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))\n",
    "logits = [tf.matmul(decoder_output, W) + b for decoder_output in decoder_outputs]\n",
    "predictions = [tf.nn.softmax(logit) for logit in logits]\n",
    "predictions = tf.stack(predictions, axis=1)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_as_list = tf.unstack(y, num=num_steps, axis=1)\n",
    "\n",
    "#losses and train_step\n",
    "losses = [tf.nn.sparse_softmax_cross_entropy_with_logits(labels=label, logits=logit) for \\\n",
    "          logit, label in zip(logits, y_as_list)]\n",
    "total_loss = tf.reduce_mean(losses)\n",
    "train_step = tf.train.AdagradOptimizer(learning_rate).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_network(sess, num_epochs, num_steps, state_size, verbose=True):\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    training_losses = []\n",
    "    for i in range(num_epochs):\n",
    "        training_loss = 0\n",
    "\n",
    "        if verbose:\n",
    "            print(\"\\nEPOCH\", i)\n",
    "        d = gen_data(time_steps=4, batch_size=16, num_batches=4000)\n",
    "        for j in range(4000):\n",
    "            X,Y = d.next()\n",
    "            training_state = np.zeros((batch_size, state_size))\n",
    "            tr_losses, training_loss_, training_state, _, preds_ = \\\n",
    "                sess.run([losses,\n",
    "                            total_loss,\n",
    "                            final_state,\n",
    "                            train_step,\n",
    "                            predictions],\n",
    "                            feed_dict={x:X, y:Y, init_state:training_state})\n",
    "            training_loss += training_loss_\n",
    "\n",
    "            if j % 40 == 0 and j > 0:\n",
    "                if verbose:\n",
    "                    print(\"Average loss at step\", j,\n",
    "                            \"for last 250 steps:\", training_loss / 100)\n",
    "                training_losses.append(training_loss / 100)\n",
    "                training_loss = 0\n",
    "                   \n",
    "      \n",
    "       \n",
    "    return training_losses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\\nEPOCH', 0)\n",
      "('Average loss at step', 40, 'for last 250 steps:', 0.5137751090526581)\n",
      "('Average loss at step', 80, 'for last 250 steps:', 0.3952367705106735)\n",
      "('Average loss at step', 120, 'for last 250 steps:', 0.32712442278862)\n",
      "('Average loss at step', 160, 'for last 250 steps:', 0.30322645664215087)\n",
      "('Average loss at step', 200, 'for last 250 steps:', 0.27247246146202087)\n",
      "('Average loss at step', 240, 'for last 250 steps:', 0.2515743887424469)\n",
      "('Average loss at step', 280, 'for last 250 steps:', 0.21833438277244568)\n",
      "('Average loss at step', 320, 'for last 250 steps:', 0.20353947311639786)\n",
      "('Average loss at step', 360, 'for last 250 steps:', 0.1875903296470642)\n",
      "('Average loss at step', 400, 'for last 250 steps:', 0.17079355984926223)\n",
      "('Average loss at step', 440, 'for last 250 steps:', 0.1655852222442627)\n",
      "('Average loss at step', 480, 'for last 250 steps:', 0.148590387403965)\n",
      "('Average loss at step', 520, 'for last 250 steps:', 0.1499617001414299)\n",
      "('Average loss at step', 560, 'for last 250 steps:', 0.13978133246302604)\n",
      "('Average loss at step', 600, 'for last 250 steps:', 0.1310170128941536)\n",
      "('Average loss at step', 640, 'for last 250 steps:', 0.12890985414385794)\n",
      "('Average loss at step', 680, 'for last 250 steps:', 0.13054050073027612)\n",
      "('Average loss at step', 720, 'for last 250 steps:', 0.12037063956260681)\n",
      "('Average loss at step', 760, 'for last 250 steps:', 0.11588617026805878)\n",
      "('Average loss at step', 800, 'for last 250 steps:', 0.1143405532836914)\n",
      "('Average loss at step', 840, 'for last 250 steps:', 0.10447396382689476)\n",
      "('Average loss at step', 880, 'for last 250 steps:', 0.11282336309552193)\n",
      "('Average loss at step', 920, 'for last 250 steps:', 0.1054836118221283)\n",
      "('Average loss at step', 960, 'for last 250 steps:', 0.10601756989955902)\n",
      "('Average loss at step', 1000, 'for last 250 steps:', 0.10072215750813485)\n",
      "('Average loss at step', 1040, 'for last 250 steps:', 0.09984255716204643)\n",
      "('Average loss at step', 1080, 'for last 250 steps:', 0.09581932291388512)\n",
      "('Average loss at step', 1120, 'for last 250 steps:', 0.09512271106243134)\n",
      "('Average loss at step', 1160, 'for last 250 steps:', 0.09081281408667564)\n",
      "('Average loss at step', 1200, 'for last 250 steps:', 0.09023287057876588)\n",
      "('Average loss at step', 1240, 'for last 250 steps:', 0.08941656686365604)\n",
      "('Average loss at step', 1280, 'for last 250 steps:', 0.08718661710619927)\n",
      "('Average loss at step', 1320, 'for last 250 steps:', 0.0907158075273037)\n",
      "('Average loss at step', 1360, 'for last 250 steps:', 0.09490430355072021)\n",
      "('Average loss at step', 1400, 'for last 250 steps:', 0.08943710088729859)\n",
      "('Average loss at step', 1440, 'for last 250 steps:', 0.07912374183535575)\n",
      "('Average loss at step', 1480, 'for last 250 steps:', 0.07807707205414773)\n",
      "('Average loss at step', 1520, 'for last 250 steps:', 0.07927010744810105)\n",
      "('Average loss at step', 1560, 'for last 250 steps:', 0.08133760012686253)\n",
      "('Average loss at step', 1600, 'for last 250 steps:', 0.07904587060213089)\n",
      "('Average loss at step', 1640, 'for last 250 steps:', 0.08121432177722454)\n",
      "('Average loss at step', 1680, 'for last 250 steps:', 0.07861205279827117)\n",
      "('Average loss at step', 1720, 'for last 250 steps:', 0.07688016593456268)\n",
      "('Average loss at step', 1760, 'for last 250 steps:', 0.07998757429420948)\n",
      "('Average loss at step', 1800, 'for last 250 steps:', 0.07484820947051048)\n",
      "('Average loss at step', 1840, 'for last 250 steps:', 0.07310905322432518)\n",
      "('Average loss at step', 1880, 'for last 250 steps:', 0.07421962782740593)\n",
      "('Average loss at step', 1920, 'for last 250 steps:', 0.06662777565419674)\n",
      "('Average loss at step', 1960, 'for last 250 steps:', 0.07410514287650585)\n",
      "('Average loss at step', 2000, 'for last 250 steps:', 0.07242576949298382)\n",
      "('Average loss at step', 2040, 'for last 250 steps:', 0.06974026583135128)\n",
      "('Average loss at step', 2080, 'for last 250 steps:', 0.07441023357212544)\n",
      "('Average loss at step', 2120, 'for last 250 steps:', 0.07341356724500656)\n",
      "('Average loss at step', 2160, 'for last 250 steps:', 0.06797524578869343)\n",
      "('Average loss at step', 2200, 'for last 250 steps:', 0.06494423434138298)\n",
      "('Average loss at step', 2240, 'for last 250 steps:', 0.06976973481476306)\n",
      "('Average loss at step', 2280, 'for last 250 steps:', 0.06704022638499736)\n",
      "('Average loss at step', 2320, 'for last 250 steps:', 0.0666955527663231)\n",
      "('Average loss at step', 2360, 'for last 250 steps:', 0.06279462702572346)\n",
      "('Average loss at step', 2400, 'for last 250 steps:', 0.07167151294648648)\n",
      "('Average loss at step', 2440, 'for last 250 steps:', 0.06689903698861599)\n",
      "('Average loss at step', 2480, 'for last 250 steps:', 0.0636359679698944)\n",
      "('Average loss at step', 2520, 'for last 250 steps:', 0.07463793642818928)\n",
      "('Average loss at step', 2560, 'for last 250 steps:', 0.06542382650077343)\n",
      "('Average loss at step', 2600, 'for last 250 steps:', 0.06221052847802639)\n",
      "('Average loss at step', 2640, 'for last 250 steps:', 0.06479738004505635)\n",
      "('Average loss at step', 2680, 'for last 250 steps:', 0.06223769344389438)\n",
      "('Average loss at step', 2720, 'for last 250 steps:', 0.06722105555236339)\n",
      "('Average loss at step', 2760, 'for last 250 steps:', 0.06342001903802157)\n",
      "('Average loss at step', 2800, 'for last 250 steps:', 0.06268603891134263)\n",
      "('Average loss at step', 2840, 'for last 250 steps:', 0.05871226146817207)\n",
      "('Average loss at step', 2880, 'for last 250 steps:', 0.061556608974933626)\n",
      "('Average loss at step', 2920, 'for last 250 steps:', 0.06104373387992382)\n",
      "('Average loss at step', 2960, 'for last 250 steps:', 0.062452387809753415)\n",
      "('Average loss at step', 3000, 'for last 250 steps:', 0.06601067259907722)\n",
      "('Average loss at step', 3040, 'for last 250 steps:', 0.06363752603530884)\n",
      "('Average loss at step', 3080, 'for last 250 steps:', 0.05838749125599861)\n",
      "('Average loss at step', 3120, 'for last 250 steps:', 0.057324680984020236)\n",
      "('Average loss at step', 3160, 'for last 250 steps:', 0.06002169921994209)\n",
      "('Average loss at step', 3200, 'for last 250 steps:', 0.055504686385393145)\n",
      "('Average loss at step', 3240, 'for last 250 steps:', 0.06276126947253942)\n",
      "('Average loss at step', 3280, 'for last 250 steps:', 0.060196053609251975)\n",
      "('Average loss at step', 3320, 'for last 250 steps:', 0.05944012142717838)\n",
      "('Average loss at step', 3360, 'for last 250 steps:', 0.056499398723244665)\n",
      "('Average loss at step', 3400, 'for last 250 steps:', 0.059266069903969765)\n",
      "('Average loss at step', 3440, 'for last 250 steps:', 0.05518875028938055)\n",
      "('Average loss at step', 3480, 'for last 250 steps:', 0.0592381314188242)\n",
      "('Average loss at step', 3520, 'for last 250 steps:', 0.05596415348351002)\n",
      "('Average loss at step', 3560, 'for last 250 steps:', 0.06419764637947083)\n",
      "('Average loss at step', 3600, 'for last 250 steps:', 0.05322451069951058)\n",
      "('Average loss at step', 3640, 'for last 250 steps:', 0.05896209493279457)\n",
      "('Average loss at step', 3680, 'for last 250 steps:', 0.060059170201420785)\n",
      "('Average loss at step', 3720, 'for last 250 steps:', 0.05875566631555557)\n",
      "('Average loss at step', 3760, 'for last 250 steps:', 0.054349168613553045)\n",
      "('Average loss at step', 3800, 'for last 250 steps:', 0.05250341184437275)\n",
      "('Average loss at step', 3840, 'for last 250 steps:', 0.06055309742689133)\n",
      "('Average loss at step', 3880, 'for last 250 steps:', 0.0597470161318779)\n",
      "('Average loss at step', 3920, 'for last 250 steps:', 0.06716072477400303)\n",
      "('Average loss at step', 3960, 'for last 250 steps:', 0.06298237435519695)\n",
      "('\\nEPOCH', 1)\n",
      "('Average loss at step', 40, 'for last 250 steps:', 0.056154809296131133)\n",
      "('Average loss at step', 80, 'for last 250 steps:', 0.05311521112918854)\n",
      "('Average loss at step', 120, 'for last 250 steps:', 0.05282372131943703)\n",
      "('Average loss at step', 160, 'for last 250 steps:', 0.053883385211229325)\n",
      "('Average loss at step', 200, 'for last 250 steps:', 0.05539712924510241)\n",
      "('Average loss at step', 240, 'for last 250 steps:', 0.04845324244350195)\n",
      "('Average loss at step', 280, 'for last 250 steps:', 0.05220541782677174)\n",
      "('Average loss at step', 320, 'for last 250 steps:', 0.053832466676831246)\n",
      "('Average loss at step', 360, 'for last 250 steps:', 0.04957875352352858)\n",
      "('Average loss at step', 400, 'for last 250 steps:', 0.04823193170130253)\n",
      "('Average loss at step', 440, 'for last 250 steps:', 0.05552233822643757)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Average loss at step', 480, 'for last 250 steps:', 0.05191352628171444)\n",
      "('Average loss at step', 520, 'for last 250 steps:', 0.050042710341513155)\n",
      "('Average loss at step', 560, 'for last 250 steps:', 0.05850351855158806)\n",
      "('Average loss at step', 600, 'for last 250 steps:', 0.05327564284205437)\n",
      "('Average loss at step', 640, 'for last 250 steps:', 0.05384358257055283)\n",
      "('Average loss at step', 680, 'for last 250 steps:', 0.051252069249749184)\n",
      "('Average loss at step', 720, 'for last 250 steps:', 0.04870166733860969)\n",
      "('Average loss at step', 760, 'for last 250 steps:', 0.051363707780838014)\n",
      "('Average loss at step', 800, 'for last 250 steps:', 0.05190787270665169)\n",
      "('Average loss at step', 840, 'for last 250 steps:', 0.04945726126432419)\n",
      "('Average loss at step', 880, 'for last 250 steps:', 0.04927842631936073)\n",
      "('Average loss at step', 920, 'for last 250 steps:', 0.05122121818363667)\n",
      "('Average loss at step', 960, 'for last 250 steps:', 0.05160583961755037)\n",
      "('Average loss at step', 1000, 'for last 250 steps:', 0.05434683185070753)\n",
      "('Average loss at step', 1040, 'for last 250 steps:', 0.045721454620361326)\n",
      "('Average loss at step', 1080, 'for last 250 steps:', 0.05229493454098701)\n",
      "('Average loss at step', 1120, 'for last 250 steps:', 0.052491302415728566)\n",
      "('Average loss at step', 1160, 'for last 250 steps:', 0.04938970092684031)\n",
      "('Average loss at step', 1200, 'for last 250 steps:', 0.04838367238640785)\n",
      "('Average loss at step', 1240, 'for last 250 steps:', 0.048659446835517886)\n",
      "('Average loss at step', 1280, 'for last 250 steps:', 0.05008719503879547)\n",
      "('Average loss at step', 1320, 'for last 250 steps:', 0.05117607280611992)\n",
      "('Average loss at step', 1360, 'for last 250 steps:', 0.04786953505128622)\n",
      "('Average loss at step', 1400, 'for last 250 steps:', 0.05062186997383833)\n",
      "('Average loss at step', 1440, 'for last 250 steps:', 0.05265379756689072)\n",
      "('Average loss at step', 1480, 'for last 250 steps:', 0.0460588451102376)\n",
      "('Average loss at step', 1520, 'for last 250 steps:', 0.05249383084475994)\n",
      "('Average loss at step', 1560, 'for last 250 steps:', 0.05323831416666508)\n",
      "('Average loss at step', 1600, 'for last 250 steps:', 0.050586741752922534)\n",
      "('Average loss at step', 1640, 'for last 250 steps:', 0.04941198885440826)\n",
      "('Average loss at step', 1680, 'for last 250 steps:', 0.05462358746677637)\n",
      "('Average loss at step', 1720, 'for last 250 steps:', 0.052401241958141324)\n",
      "('Average loss at step', 1760, 'for last 250 steps:', 0.04582975044846535)\n",
      "('Average loss at step', 1800, 'for last 250 steps:', 0.050383708365261556)\n",
      "('Average loss at step', 1840, 'for last 250 steps:', 0.047153405472636224)\n",
      "('Average loss at step', 1880, 'for last 250 steps:', 0.045696239732205865)\n",
      "('Average loss at step', 1920, 'for last 250 steps:', 0.051044484116137025)\n",
      "('Average loss at step', 1960, 'for last 250 steps:', 0.04707658067345619)\n",
      "('Average loss at step', 2000, 'for last 250 steps:', 0.046706143729388715)\n",
      "('Average loss at step', 2040, 'for last 250 steps:', 0.05081578977406025)\n",
      "('Average loss at step', 2080, 'for last 250 steps:', 0.048738525211811067)\n",
      "('Average loss at step', 2120, 'for last 250 steps:', 0.04625383418053389)\n",
      "('Average loss at step', 2160, 'for last 250 steps:', 0.04876662872731686)\n",
      "('Average loss at step', 2200, 'for last 250 steps:', 0.04515423621982336)\n",
      "('Average loss at step', 2240, 'for last 250 steps:', 0.04132374055683613)\n",
      "('Average loss at step', 2280, 'for last 250 steps:', 0.041101818159222606)\n",
      "('Average loss at step', 2320, 'for last 250 steps:', 0.04761489048600197)\n",
      "('Average loss at step', 2360, 'for last 250 steps:', 0.047994659133255485)\n",
      "('Average loss at step', 2400, 'for last 250 steps:', 0.047187332883477213)\n",
      "('Average loss at step', 2440, 'for last 250 steps:', 0.04952575333416462)\n",
      "('Average loss at step', 2480, 'for last 250 steps:', 0.046820304095745086)\n",
      "('Average loss at step', 2520, 'for last 250 steps:', 0.048477406166493896)\n",
      "('Average loss at step', 2560, 'for last 250 steps:', 0.04923348154872656)\n",
      "('Average loss at step', 2600, 'for last 250 steps:', 0.05111102320253849)\n",
      "('Average loss at step', 2640, 'for last 250 steps:', 0.05028668571263552)\n",
      "('Average loss at step', 2680, 'for last 250 steps:', 0.04746042750775814)\n",
      "('Average loss at step', 2720, 'for last 250 steps:', 0.04938565634191036)\n",
      "('Average loss at step', 2760, 'for last 250 steps:', 0.04567491181194782)\n",
      "('Average loss at step', 2800, 'for last 250 steps:', 0.040669520013034345)\n",
      "('Average loss at step', 2840, 'for last 250 steps:', 0.05078195136040449)\n",
      "('Average loss at step', 2880, 'for last 250 steps:', 0.048945041559636596)\n",
      "('Average loss at step', 2920, 'for last 250 steps:', 0.04769414022564888)\n",
      "('Average loss at step', 2960, 'for last 250 steps:', 0.043937255293130875)\n",
      "('Average loss at step', 3000, 'for last 250 steps:', 0.05236763510853052)\n",
      "('Average loss at step', 3040, 'for last 250 steps:', 0.05161565512418747)\n",
      "('Average loss at step', 3080, 'for last 250 steps:', 0.047472676895558834)\n",
      "('Average loss at step', 3120, 'for last 250 steps:', 0.05031485687941313)\n",
      "('Average loss at step', 3160, 'for last 250 steps:', 0.042388304248452184)\n",
      "('Average loss at step', 3200, 'for last 250 steps:', 0.050300392024219036)\n",
      "('Average loss at step', 3240, 'for last 250 steps:', 0.04810322567820549)\n",
      "('Average loss at step', 3280, 'for last 250 steps:', 0.04777367927134037)\n",
      "('Average loss at step', 3320, 'for last 250 steps:', 0.04236733682453633)\n",
      "('Average loss at step', 3360, 'for last 250 steps:', 0.04373793847858906)\n",
      "('Average loss at step', 3400, 'for last 250 steps:', 0.04664481662213802)\n",
      "('Average loss at step', 3440, 'for last 250 steps:', 0.05172224760055542)\n",
      "('Average loss at step', 3480, 'for last 250 steps:', 0.05084202360361814)\n",
      "('Average loss at step', 3520, 'for last 250 steps:', 0.04005510743707418)\n",
      "('Average loss at step', 3560, 'for last 250 steps:', 0.04486994381994009)\n",
      "('Average loss at step', 3600, 'for last 250 steps:', 0.04345293235033751)\n",
      "('Average loss at step', 3640, 'for last 250 steps:', 0.04418321583420038)\n",
      "('Average loss at step', 3680, 'for last 250 steps:', 0.044981793239712714)\n",
      "('Average loss at step', 3720, 'for last 250 steps:', 0.04783766575157642)\n",
      "('Average loss at step', 3760, 'for last 250 steps:', 0.04865135952830315)\n",
      "('Average loss at step', 3800, 'for last 250 steps:', 0.0449939851090312)\n",
      "('Average loss at step', 3840, 'for last 250 steps:', 0.047855457179248335)\n",
      "('Average loss at step', 3880, 'for last 250 steps:', 0.04954786065965891)\n",
      "('Average loss at step', 3920, 'for last 250 steps:', 0.0401526440680027)\n",
      "('Average loss at step', 3960, 'for last 250 steps:', 0.04296796433627605)\n",
      "('\\nEPOCH', 2)\n",
      "('Average loss at step', 40, 'for last 250 steps:', 0.0515400580316782)\n",
      "('Average loss at step', 80, 'for last 250 steps:', 0.04007264919579029)\n",
      "('Average loss at step', 120, 'for last 250 steps:', 0.05097788397222757)\n",
      "('Average loss at step', 160, 'for last 250 steps:', 0.045184195674955845)\n",
      "('Average loss at step', 200, 'for last 250 steps:', 0.039276339635252955)\n",
      "('Average loss at step', 240, 'for last 250 steps:', 0.03870687693357468)\n",
      "('Average loss at step', 280, 'for last 250 steps:', 0.038920524343848226)\n",
      "('Average loss at step', 320, 'for last 250 steps:', 0.04590829569846391)\n",
      "('Average loss at step', 360, 'for last 250 steps:', 0.045992437861859796)\n",
      "('Average loss at step', 400, 'for last 250 steps:', 0.04024657975882292)\n",
      "('Average loss at step', 440, 'for last 250 steps:', 0.042384865880012515)\n",
      "('Average loss at step', 480, 'for last 250 steps:', 0.04149183202534914)\n",
      "('Average loss at step', 520, 'for last 250 steps:', 0.04314621068537235)\n",
      "('Average loss at step', 560, 'for last 250 steps:', 0.04423857670277357)\n",
      "('Average loss at step', 600, 'for last 250 steps:', 0.04633205533027649)\n",
      "('Average loss at step', 640, 'for last 250 steps:', 0.04277810283005237)\n",
      "('Average loss at step', 680, 'for last 250 steps:', 0.04351671036332846)\n",
      "('Average loss at step', 720, 'for last 250 steps:', 0.0436272519826889)\n",
      "('Average loss at step', 760, 'for last 250 steps:', 0.04288113255053758)\n",
      "('Average loss at step', 800, 'for last 250 steps:', 0.04085379336029291)\n",
      "('Average loss at step', 840, 'for last 250 steps:', 0.03900755237787962)\n",
      "('Average loss at step', 880, 'for last 250 steps:', 0.04331306010484695)\n",
      "('Average loss at step', 920, 'for last 250 steps:', 0.04202988788485527)\n",
      "('Average loss at step', 960, 'for last 250 steps:', 0.04111135829240084)\n",
      "('Average loss at step', 1000, 'for last 250 steps:', 0.041203748062253)\n",
      "('Average loss at step', 1040, 'for last 250 steps:', 0.048860356882214545)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Average loss at step', 1080, 'for last 250 steps:', 0.04406982615590096)\n",
      "('Average loss at step', 1120, 'for last 250 steps:', 0.038429759442806244)\n",
      "('Average loss at step', 1160, 'for last 250 steps:', 0.041648577228188516)\n",
      "('Average loss at step', 1200, 'for last 250 steps:', 0.041688421890139576)\n",
      "('Average loss at step', 1240, 'for last 250 steps:', 0.04099574662744999)\n",
      "('Average loss at step', 1280, 'for last 250 steps:', 0.043839990049600604)\n",
      "('Average loss at step', 1320, 'for last 250 steps:', 0.03741818334907293)\n",
      "('Average loss at step', 1360, 'for last 250 steps:', 0.04456681158393622)\n",
      "('Average loss at step', 1400, 'for last 250 steps:', 0.04338780730962753)\n",
      "('Average loss at step', 1440, 'for last 250 steps:', 0.03712723821401596)\n",
      "('Average loss at step', 1480, 'for last 250 steps:', 0.041239724103361367)\n",
      "('Average loss at step', 1520, 'for last 250 steps:', 0.049437598884105684)\n",
      "('Average loss at step', 1560, 'for last 250 steps:', 0.044204819314181805)\n",
      "('Average loss at step', 1600, 'for last 250 steps:', 0.04039115689694881)\n",
      "('Average loss at step', 1640, 'for last 250 steps:', 0.04103350631892681)\n",
      "('Average loss at step', 1680, 'for last 250 steps:', 0.043680494166910645)\n",
      "('Average loss at step', 1720, 'for last 250 steps:', 0.04593272250145674)\n",
      "('Average loss at step', 1760, 'for last 250 steps:', 0.04227379579097033)\n",
      "('Average loss at step', 1800, 'for last 250 steps:', 0.04287532020360232)\n",
      "('Average loss at step', 1840, 'for last 250 steps:', 0.036373688653111455)\n",
      "('Average loss at step', 1880, 'for last 250 steps:', 0.04696084048599005)\n",
      "('Average loss at step', 1920, 'for last 250 steps:', 0.039286407977342605)\n",
      "('Average loss at step', 1960, 'for last 250 steps:', 0.04242685422301293)\n",
      "('Average loss at step', 2000, 'for last 250 steps:', 0.04468270264565945)\n",
      "('Average loss at step', 2040, 'for last 250 steps:', 0.03926472106948495)\n",
      "('Average loss at step', 2080, 'for last 250 steps:', 0.04357128407806158)\n",
      "('Average loss at step', 2120, 'for last 250 steps:', 0.04247289024293423)\n",
      "('Average loss at step', 2160, 'for last 250 steps:', 0.04322142221033573)\n",
      "('Average loss at step', 2200, 'for last 250 steps:', 0.04114158853888512)\n",
      "('Average loss at step', 2240, 'for last 250 steps:', 0.04351055052131415)\n",
      "('Average loss at step', 2280, 'for last 250 steps:', 0.041012516282498836)\n",
      "('Average loss at step', 2320, 'for last 250 steps:', 0.046263987459242345)\n",
      "('Average loss at step', 2360, 'for last 250 steps:', 0.04510027684271336)\n",
      "('Average loss at step', 2400, 'for last 250 steps:', 0.04000964872539044)\n",
      "('Average loss at step', 2440, 'for last 250 steps:', 0.04253526993095875)\n",
      "('Average loss at step', 2480, 'for last 250 steps:', 0.037719397097826006)\n",
      "('Average loss at step', 2520, 'for last 250 steps:', 0.04379695188254118)\n",
      "('Average loss at step', 2560, 'for last 250 steps:', 0.05344938799738884)\n",
      "('Average loss at step', 2600, 'for last 250 steps:', 0.041400618217885495)\n",
      "('Average loss at step', 2640, 'for last 250 steps:', 0.035978405885398385)\n",
      "('Average loss at step', 2680, 'for last 250 steps:', 0.03852887138724327)\n",
      "('Average loss at step', 2720, 'for last 250 steps:', 0.04781028501689434)\n",
      "('Average loss at step', 2760, 'for last 250 steps:', 0.04182742517441511)\n",
      "('Average loss at step', 2800, 'for last 250 steps:', 0.04204456582665443)\n",
      "('Average loss at step', 2840, 'for last 250 steps:', 0.046065020523965355)\n",
      "('Average loss at step', 2880, 'for last 250 steps:', 0.046851918026804926)\n",
      "('Average loss at step', 2920, 'for last 250 steps:', 0.04343211570754647)\n",
      "('Average loss at step', 2960, 'for last 250 steps:', 0.04479055153205991)\n",
      "('Average loss at step', 3000, 'for last 250 steps:', 0.0417674995213747)\n",
      "('Average loss at step', 3040, 'for last 250 steps:', 0.04214783065021038)\n",
      "('Average loss at step', 3080, 'for last 250 steps:', 0.04384257452562451)\n",
      "('Average loss at step', 3120, 'for last 250 steps:', 0.04084539659321308)\n",
      "('Average loss at step', 3160, 'for last 250 steps:', 0.03815112568438053)\n",
      "('Average loss at step', 3200, 'for last 250 steps:', 0.04199889715760946)\n",
      "('Average loss at step', 3240, 'for last 250 steps:', 0.03999595552682877)\n",
      "('Average loss at step', 3280, 'for last 250 steps:', 0.043754137307405475)\n",
      "('Average loss at step', 3320, 'for last 250 steps:', 0.038311957269907)\n",
      "('Average loss at step', 3360, 'for last 250 steps:', 0.04277526471763849)\n",
      "('Average loss at step', 3400, 'for last 250 steps:', 0.042358799912035464)\n",
      "('Average loss at step', 3440, 'for last 250 steps:', 0.037789458241313696)\n",
      "('Average loss at step', 3480, 'for last 250 steps:', 0.04184404253959656)\n",
      "('Average loss at step', 3520, 'for last 250 steps:', 0.03896921768784523)\n",
      "('Average loss at step', 3560, 'for last 250 steps:', 0.038307251185178755)\n",
      "('Average loss at step', 3600, 'for last 250 steps:', 0.041779345236718654)\n",
      "('Average loss at step', 3640, 'for last 250 steps:', 0.0420477669686079)\n",
      "('Average loss at step', 3680, 'for last 250 steps:', 0.03666793823242188)\n",
      "('Average loss at step', 3720, 'for last 250 steps:', 0.04055871285498142)\n",
      "('Average loss at step', 3760, 'for last 250 steps:', 0.037361656092107294)\n",
      "('Average loss at step', 3800, 'for last 250 steps:', 0.045933242179453375)\n",
      "('Average loss at step', 3840, 'for last 250 steps:', 0.0431899032369256)\n",
      "('Average loss at step', 3880, 'for last 250 steps:', 0.04391347374767065)\n",
      "('Average loss at step', 3920, 'for last 250 steps:', 0.039671944044530394)\n",
      "('Average loss at step', 3960, 'for last 250 steps:', 0.03795677360147238)\n",
      "('\\nEPOCH', 3)\n",
      "('Average loss at step', 40, 'for last 250 steps:', 0.044087347202003004)\n",
      "('Average loss at step', 80, 'for last 250 steps:', 0.043485824279487134)\n",
      "('Average loss at step', 120, 'for last 250 steps:', 0.04049358211457729)\n",
      "('Average loss at step', 160, 'for last 250 steps:', 0.03879837490618229)\n",
      "('Average loss at step', 200, 'for last 250 steps:', 0.04241231437772512)\n",
      "('Average loss at step', 240, 'for last 250 steps:', 0.04679369219578802)\n",
      "('Average loss at step', 280, 'for last 250 steps:', 0.03674023773521185)\n",
      "('Average loss at step', 320, 'for last 250 steps:', 0.03922655694186687)\n",
      "('Average loss at step', 360, 'for last 250 steps:', 0.044238284789025785)\n",
      "('Average loss at step', 400, 'for last 250 steps:', 0.03892644554376602)\n",
      "('Average loss at step', 440, 'for last 250 steps:', 0.04046464402228594)\n",
      "('Average loss at step', 480, 'for last 250 steps:', 0.043017158918082714)\n",
      "('Average loss at step', 520, 'for last 250 steps:', 0.040131972935050726)\n",
      "('Average loss at step', 560, 'for last 250 steps:', 0.03903617490082979)\n",
      "('Average loss at step', 600, 'for last 250 steps:', 0.03993296911939979)\n",
      "('Average loss at step', 640, 'for last 250 steps:', 0.04332466047257185)\n",
      "('Average loss at step', 680, 'for last 250 steps:', 0.04434533566236496)\n",
      "('Average loss at step', 720, 'for last 250 steps:', 0.03838212698698044)\n",
      "('Average loss at step', 760, 'for last 250 steps:', 0.038649414870887994)\n",
      "('Average loss at step', 800, 'for last 250 steps:', 0.03772760275751352)\n",
      "('Average loss at step', 840, 'for last 250 steps:', 0.03894287765026092)\n",
      "('Average loss at step', 880, 'for last 250 steps:', 0.04172862987965345)\n",
      "('Average loss at step', 920, 'for last 250 steps:', 0.03811670906841755)\n",
      "('Average loss at step', 960, 'for last 250 steps:', 0.035025533810257914)\n",
      "('Average loss at step', 1000, 'for last 250 steps:', 0.04181165292859077)\n",
      "('Average loss at step', 1040, 'for last 250 steps:', 0.04383850432932377)\n",
      "('Average loss at step', 1080, 'for last 250 steps:', 0.03727438319474459)\n",
      "('Average loss at step', 1120, 'for last 250 steps:', 0.03742933992296457)\n",
      "('Average loss at step', 1160, 'for last 250 steps:', 0.04372172474861145)\n",
      "('Average loss at step', 1200, 'for last 250 steps:', 0.04135075345635414)\n",
      "('Average loss at step', 1240, 'for last 250 steps:', 0.04159044284373522)\n",
      "('Average loss at step', 1280, 'for last 250 steps:', 0.03742418866604567)\n",
      "('Average loss at step', 1320, 'for last 250 steps:', 0.042519900780171156)\n",
      "('Average loss at step', 1360, 'for last 250 steps:', 0.04053691327571869)\n",
      "('Average loss at step', 1400, 'for last 250 steps:', 0.03852458316832781)\n",
      "('Average loss at step', 1440, 'for last 250 steps:', 0.03909727849066257)\n",
      "('Average loss at step', 1480, 'for last 250 steps:', 0.03631600806489587)\n",
      "('Average loss at step', 1520, 'for last 250 steps:', 0.041455721259117125)\n",
      "('Average loss at step', 1560, 'for last 250 steps:', 0.0389547935500741)\n",
      "('Average loss at step', 1600, 'for last 250 steps:', 0.03999184314161539)\n",
      "('Average loss at step', 1640, 'for last 250 steps:', 0.039322628416121004)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Average loss at step', 1680, 'for last 250 steps:', 0.03576546832919121)\n",
      "('Average loss at step', 1720, 'for last 250 steps:', 0.03687184466049075)\n",
      "('Average loss at step', 1760, 'for last 250 steps:', 0.034542735256254675)\n",
      "('Average loss at step', 1800, 'for last 250 steps:', 0.03454742547124624)\n",
      "('Average loss at step', 1840, 'for last 250 steps:', 0.03719648968428373)\n",
      "('Average loss at step', 1880, 'for last 250 steps:', 0.039720839262008666)\n",
      "('Average loss at step', 1920, 'for last 250 steps:', 0.040967750810086724)\n",
      "('Average loss at step', 1960, 'for last 250 steps:', 0.0363571435585618)\n",
      "('Average loss at step', 2000, 'for last 250 steps:', 0.04004433635622263)\n",
      "('Average loss at step', 2040, 'for last 250 steps:', 0.03465532220900059)\n",
      "('Average loss at step', 2080, 'for last 250 steps:', 0.038483396507799625)\n",
      "('Average loss at step', 2120, 'for last 250 steps:', 0.04241416998207569)\n",
      "('Average loss at step', 2160, 'for last 250 steps:', 0.04218857686966657)\n",
      "('Average loss at step', 2200, 'for last 250 steps:', 0.03591576471924782)\n",
      "('Average loss at step', 2240, 'for last 250 steps:', 0.04483085544779897)\n",
      "('Average loss at step', 2280, 'for last 250 steps:', 0.03635979764163494)\n",
      "('Average loss at step', 2320, 'for last 250 steps:', 0.03853917418047786)\n",
      "('Average loss at step', 2360, 'for last 250 steps:', 0.04043273776769638)\n",
      "('Average loss at step', 2400, 'for last 250 steps:', 0.03808870226144791)\n",
      "('Average loss at step', 2440, 'for last 250 steps:', 0.03545938450843096)\n",
      "('Average loss at step', 2480, 'for last 250 steps:', 0.03559174196794629)\n",
      "('Average loss at step', 2520, 'for last 250 steps:', 0.036427458710968495)\n",
      "('Average loss at step', 2560, 'for last 250 steps:', 0.03931193444877863)\n",
      "('Average loss at step', 2600, 'for last 250 steps:', 0.03400444805622101)\n",
      "('Average loss at step', 2640, 'for last 250 steps:', 0.03719985563308001)\n",
      "('Average loss at step', 2680, 'for last 250 steps:', 0.03532288882881403)\n",
      "('Average loss at step', 2720, 'for last 250 steps:', 0.035501266196370125)\n",
      "('Average loss at step', 2760, 'for last 250 steps:', 0.03941668059676886)\n",
      "('Average loss at step', 2800, 'for last 250 steps:', 0.04352696642279625)\n",
      "('Average loss at step', 2840, 'for last 250 steps:', 0.04027910077944398)\n",
      "('Average loss at step', 2880, 'for last 250 steps:', 0.03523795526474714)\n",
      "('Average loss at step', 2920, 'for last 250 steps:', 0.036258262693881986)\n",
      "('Average loss at step', 2960, 'for last 250 steps:', 0.03327571429312229)\n",
      "('Average loss at step', 3000, 'for last 250 steps:', 0.04036170959472656)\n",
      "('Average loss at step', 3040, 'for last 250 steps:', 0.03494000049307942)\n",
      "('Average loss at step', 3080, 'for last 250 steps:', 0.03523317284882069)\n",
      "('Average loss at step', 3120, 'for last 250 steps:', 0.039575552232563496)\n",
      "('Average loss at step', 3160, 'for last 250 steps:', 0.04519879996776581)\n",
      "('Average loss at step', 3200, 'for last 250 steps:', 0.03886739443987608)\n",
      "('Average loss at step', 3240, 'for last 250 steps:', 0.03368716035038233)\n",
      "('Average loss at step', 3280, 'for last 250 steps:', 0.03583622448146343)\n",
      "('Average loss at step', 3320, 'for last 250 steps:', 0.03542378585785627)\n",
      "('Average loss at step', 3360, 'for last 250 steps:', 0.035606114268302916)\n",
      "('Average loss at step', 3400, 'for last 250 steps:', 0.03308776490390301)\n",
      "('Average loss at step', 3440, 'for last 250 steps:', 0.040359257981181146)\n",
      "('Average loss at step', 3480, 'for last 250 steps:', 0.0361480899900198)\n",
      "('Average loss at step', 3520, 'for last 250 steps:', 0.036521503888070586)\n",
      "('Average loss at step', 3560, 'for last 250 steps:', 0.03461993522942066)\n",
      "('Average loss at step', 3600, 'for last 250 steps:', 0.037283938117325305)\n",
      "('Average loss at step', 3640, 'for last 250 steps:', 0.03508805047720671)\n",
      "('Average loss at step', 3680, 'for last 250 steps:', 0.03674159979447723)\n",
      "('Average loss at step', 3720, 'for last 250 steps:', 0.0385058768466115)\n",
      "('Average loss at step', 3760, 'for last 250 steps:', 0.03675234392285347)\n",
      "('Average loss at step', 3800, 'for last 250 steps:', 0.04066939629614353)\n",
      "('Average loss at step', 3840, 'for last 250 steps:', 0.04109812911599874)\n",
      "('Average loss at step', 3880, 'for last 250 steps:', 0.03342103313654661)\n",
      "('Average loss at step', 3920, 'for last 250 steps:', 0.039379689283668994)\n",
      "('Average loss at step', 3960, 'for last 250 steps:', 0.03944810658693314)\n",
      "('\\nEPOCH', 4)\n",
      "('Average loss at step', 40, 'for last 250 steps:', 0.036844287179410455)\n",
      "('Average loss at step', 80, 'for last 250 steps:', 0.039257463477551935)\n",
      "('Average loss at step', 120, 'for last 250 steps:', 0.034183087553828956)\n",
      "('Average loss at step', 160, 'for last 250 steps:', 0.04395964242517948)\n",
      "('Average loss at step', 200, 'for last 250 steps:', 0.036416912898421284)\n",
      "('Average loss at step', 240, 'for last 250 steps:', 0.0418254036270082)\n",
      "('Average loss at step', 280, 'for last 250 steps:', 0.03510679613798857)\n",
      "('Average loss at step', 320, 'for last 250 steps:', 0.03911192875355482)\n",
      "('Average loss at step', 360, 'for last 250 steps:', 0.03602180894464255)\n",
      "('Average loss at step', 400, 'for last 250 steps:', 0.03674938417971134)\n",
      "('Average loss at step', 440, 'for last 250 steps:', 0.03367103669792414)\n",
      "('Average loss at step', 480, 'for last 250 steps:', 0.037807402200996876)\n",
      "('Average loss at step', 520, 'for last 250 steps:', 0.038296379558742044)\n",
      "('Average loss at step', 560, 'for last 250 steps:', 0.040271872486919165)\n",
      "('Average loss at step', 600, 'for last 250 steps:', 0.04087962616235018)\n",
      "('Average loss at step', 640, 'for last 250 steps:', 0.03977789726108313)\n",
      "('Average loss at step', 680, 'for last 250 steps:', 0.03536043051630258)\n",
      "('Average loss at step', 720, 'for last 250 steps:', 0.03882299691438675)\n",
      "('Average loss at step', 760, 'for last 250 steps:', 0.04229697808623314)\n",
      "('Average loss at step', 800, 'for last 250 steps:', 0.03866054952144623)\n",
      "('Average loss at step', 840, 'for last 250 steps:', 0.03282688196748495)\n",
      "('Average loss at step', 880, 'for last 250 steps:', 0.034872288815677166)\n",
      "('Average loss at step', 920, 'for last 250 steps:', 0.036832643318921325)\n",
      "('Average loss at step', 960, 'for last 250 steps:', 0.041729811057448385)\n",
      "('Average loss at step', 1000, 'for last 250 steps:', 0.02771909523755312)\n",
      "('Average loss at step', 1040, 'for last 250 steps:', 0.04080511827021837)\n",
      "('Average loss at step', 1080, 'for last 250 steps:', 0.03447083465754986)\n",
      "('Average loss at step', 1120, 'for last 250 steps:', 0.034285974577069285)\n",
      "('Average loss at step', 1160, 'for last 250 steps:', 0.03535764440894127)\n",
      "('Average loss at step', 1200, 'for last 250 steps:', 0.03278570491820574)\n",
      "('Average loss at step', 1240, 'for last 250 steps:', 0.03411620423197746)\n",
      "('Average loss at step', 1280, 'for last 250 steps:', 0.03468444187194109)\n",
      "('Average loss at step', 1320, 'for last 250 steps:', 0.040731004402041436)\n",
      "('Average loss at step', 1360, 'for last 250 steps:', 0.03604015784338117)\n",
      "('Average loss at step', 1400, 'for last 250 steps:', 0.03878323081880808)\n",
      "('Average loss at step', 1440, 'for last 250 steps:', 0.038505435958504676)\n",
      "('Average loss at step', 1480, 'for last 250 steps:', 0.03499752588570118)\n",
      "('Average loss at step', 1520, 'for last 250 steps:', 0.035193824283778666)\n",
      "('Average loss at step', 1560, 'for last 250 steps:', 0.03687882211059332)\n",
      "('Average loss at step', 1600, 'for last 250 steps:', 0.03476057073101401)\n",
      "('Average loss at step', 1640, 'for last 250 steps:', 0.03140012785792351)\n",
      "('Average loss at step', 1680, 'for last 250 steps:', 0.03319529917091131)\n",
      "('Average loss at step', 1720, 'for last 250 steps:', 0.035625168196856974)\n",
      "('Average loss at step', 1760, 'for last 250 steps:', 0.03873558711260557)\n",
      "('Average loss at step', 1800, 'for last 250 steps:', 0.04097427586093545)\n",
      "('Average loss at step', 1840, 'for last 250 steps:', 0.036673664897680286)\n",
      "('Average loss at step', 1880, 'for last 250 steps:', 0.03716014832258224)\n",
      "('Average loss at step', 1920, 'for last 250 steps:', 0.03407461389899254)\n",
      "('Average loss at step', 1960, 'for last 250 steps:', 0.0355084203183651)\n",
      "('Average loss at step', 2000, 'for last 250 steps:', 0.03517777977511287)\n",
      "('Average loss at step', 2040, 'for last 250 steps:', 0.034810618739575146)\n",
      "('Average loss at step', 2080, 'for last 250 steps:', 0.03560356270521879)\n",
      "('Average loss at step', 2120, 'for last 250 steps:', 0.04110481899231672)\n",
      "('Average loss at step', 2160, 'for last 250 steps:', 0.03478334672749042)\n",
      "('Average loss at step', 2200, 'for last 250 steps:', 0.03790969472378492)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Average loss at step', 2240, 'for last 250 steps:', 0.032574925627559426)\n",
      "('Average loss at step', 2280, 'for last 250 steps:', 0.03717279925942421)\n",
      "('Average loss at step', 2320, 'for last 250 steps:', 0.03528461877256632)\n",
      "('Average loss at step', 2360, 'for last 250 steps:', 0.03174937158823013)\n",
      "('Average loss at step', 2400, 'for last 250 steps:', 0.039675374366343025)\n",
      "('Average loss at step', 2440, 'for last 250 steps:', 0.03186618110165)\n",
      "('Average loss at step', 2480, 'for last 250 steps:', 0.03560568168759346)\n",
      "('Average loss at step', 2520, 'for last 250 steps:', 0.03781978139653802)\n",
      "('Average loss at step', 2560, 'for last 250 steps:', 0.035776642486453056)\n",
      "('Average loss at step', 2600, 'for last 250 steps:', 0.03673668809235096)\n",
      "('Average loss at step', 2640, 'for last 250 steps:', 0.03823245670646429)\n",
      "('Average loss at step', 2680, 'for last 250 steps:', 0.034328334499150515)\n",
      "('Average loss at step', 2720, 'for last 250 steps:', 0.034469976108521226)\n",
      "('Average loss at step', 2760, 'for last 250 steps:', 0.033126472942531106)\n",
      "('Average loss at step', 2800, 'for last 250 steps:', 0.03461793009191751)\n",
      "('Average loss at step', 2840, 'for last 250 steps:', 0.028626104351133108)\n",
      "('Average loss at step', 2880, 'for last 250 steps:', 0.03303693551570177)\n",
      "('Average loss at step', 2920, 'for last 250 steps:', 0.034623014517128466)\n",
      "('Average loss at step', 2960, 'for last 250 steps:', 0.03791720412671566)\n",
      "('Average loss at step', 3000, 'for last 250 steps:', 0.04148015663027763)\n",
      "('Average loss at step', 3040, 'for last 250 steps:', 0.03560952577739954)\n",
      "('Average loss at step', 3080, 'for last 250 steps:', 0.03318138279020786)\n",
      "('Average loss at step', 3120, 'for last 250 steps:', 0.037262358590960505)\n",
      "('Average loss at step', 3160, 'for last 250 steps:', 0.03524494130164385)\n",
      "('Average loss at step', 3200, 'for last 250 steps:', 0.03197706922888756)\n",
      "('Average loss at step', 3240, 'for last 250 steps:', 0.03243323124945164)\n",
      "('Average loss at step', 3280, 'for last 250 steps:', 0.03194112772122026)\n",
      "('Average loss at step', 3320, 'for last 250 steps:', 0.033537844680249694)\n",
      "('Average loss at step', 3360, 'for last 250 steps:', 0.03022537613287568)\n",
      "('Average loss at step', 3400, 'for last 250 steps:', 0.033076818585395816)\n",
      "('Average loss at step', 3440, 'for last 250 steps:', 0.03356470998376608)\n",
      "('Average loss at step', 3480, 'for last 250 steps:', 0.03598776567727327)\n",
      "('Average loss at step', 3520, 'for last 250 steps:', 0.03330221915617585)\n",
      "('Average loss at step', 3560, 'for last 250 steps:', 0.03410278033465147)\n",
      "('Average loss at step', 3600, 'for last 250 steps:', 0.03490842901170254)\n",
      "('Average loss at step', 3640, 'for last 250 steps:', 0.032319310158491134)\n",
      "('Average loss at step', 3680, 'for last 250 steps:', 0.03403482537716627)\n",
      "('Average loss at step', 3720, 'for last 250 steps:', 0.033509892784059046)\n",
      "('Average loss at step', 3760, 'for last 250 steps:', 0.03529115652665496)\n",
      "('Average loss at step', 3800, 'for last 250 steps:', 0.036500968150794504)\n",
      "('Average loss at step', 3840, 'for last 250 steps:', 0.03313050553202629)\n",
      "('Average loss at step', 3880, 'for last 250 steps:', 0.03337138626724481)\n",
      "('Average loss at step', 3920, 'for last 250 steps:', 0.04275354754179716)\n",
      "('Average loss at step', 3960, 'for last 250 steps:', 0.033589024990797044)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x10d146e90>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "session = tf.Session()\n",
    "training_losses = train_network(session, 5,num_steps, state_size)\n",
    "plt.plot(training_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[[8.9985805e-12, 1.6125365e-05, 9.9998283e-01, 1.1158774e-06],\n",
      "        [1.0277406e-05, 9.8957342e-01, 1.5084824e-03, 8.9079225e-03],\n",
      "        [3.3558076e-06, 2.5288583e-04, 1.4690220e-08, 9.9974376e-01],\n",
      "        [9.9980742e-01, 1.8998302e-04, 2.1550686e-07, 2.3331434e-06]],\n",
      "\n",
      "       [[8.3424396e-08, 1.2815111e-08, 9.9623340e-01, 3.7665172e-03],\n",
      "        [6.6434893e-08, 2.4649471e-09, 7.9410552e-04, 9.9920589e-01],\n",
      "        [9.9896395e-01, 8.3149754e-04, 9.5075702e-06, 1.9507125e-04],\n",
      "        [3.0233885e-08, 9.9999988e-01, 8.2391082e-08, 2.1948290e-11]],\n",
      "\n",
      "       [[1.4286939e-02, 1.0113757e-06, 2.1256385e-03, 9.8358637e-01],\n",
      "        [9.9947828e-01, 1.9761079e-04, 1.9673827e-04, 1.2738079e-04],\n",
      "        [2.1753012e-05, 6.6064107e-01, 3.3933720e-01, 6.7603154e-09],\n",
      "        [7.6158409e-05, 9.9902165e-01, 9.0225204e-04, 1.3501499e-09]],\n",
      "\n",
      "       [[9.9496800e-01, 8.9394581e-07, 5.0309855e-03, 6.4400687e-08],\n",
      "        [2.1449121e-05, 1.5548588e-04, 9.9903011e-01, 7.9290679e-04],\n",
      "        [1.4129427e-04, 7.5004580e-03, 5.3818389e-03, 9.8697644e-01],\n",
      "        [1.6218361e-03, 9.9837035e-01, 5.0897695e-09, 7.8527664e-06]],\n",
      "\n",
      "       [[9.9609452e-01, 2.4275900e-07, 3.0393088e-03, 8.6591055e-04],\n",
      "        [4.9212016e-08, 5.2238152e-06, 4.4826373e-01, 5.5173099e-01],\n",
      "        [5.7989190e-08, 3.9711140e-06, 6.4179040e-03, 9.9357808e-01],\n",
      "        [1.0590818e-03, 9.9893552e-01, 1.1777789e-08, 5.3414701e-06]],\n",
      "\n",
      "       [[9.9883944e-01, 3.8677488e-08, 6.9650326e-09, 1.1605562e-03],\n",
      "        [1.4977411e-03, 3.6592078e-03, 2.9213138e-07, 9.9484283e-01],\n",
      "        [1.6088651e-04, 9.9818063e-01, 1.6417924e-03, 1.6743716e-05],\n",
      "        [9.4117313e-06, 6.5806785e-07, 9.9998999e-01, 5.1026765e-09]],\n",
      "\n",
      "       [[2.0455347e-01, 1.6454263e-10, 6.5041545e-09, 7.9544652e-01],\n",
      "        [1.4338966e-09, 3.7301945e-08, 1.3633956e-04, 9.9986362e-01],\n",
      "        [4.3467697e-08, 6.2845340e-03, 9.9336207e-01, 3.5337257e-04],\n",
      "        [1.1039246e-05, 9.9781764e-01, 2.1531421e-03, 1.8147846e-05]],\n",
      "\n",
      "       [[5.0119152e-03, 2.4824798e-10, 4.4502063e-10, 9.9498808e-01],\n",
      "        [9.9999762e-01, 1.0401948e-06, 8.7361091e-11, 1.3673624e-06],\n",
      "        [1.1232422e-05, 9.9992609e-01, 6.2725645e-05, 1.9717188e-12],\n",
      "        [3.0359268e-05, 2.0320278e-03, 9.9793637e-01, 1.2593879e-06]],\n",
      "\n",
      "       [[8.3246362e-01, 1.1383223e-06, 1.5136079e-01, 1.6174464e-02],\n",
      "        [5.7624696e-08, 1.5577414e-06, 2.4929759e-01, 7.5070077e-01],\n",
      "        [9.5371632e-08, 2.9731730e-06, 2.2881131e-03, 9.9770880e-01],\n",
      "        [2.6178975e-03, 9.9737728e-01, 6.1055014e-09, 4.8941070e-06]],\n",
      "\n",
      "       [[1.1353550e-05, 9.9907112e-01, 1.7303602e-08, 9.1756624e-04],\n",
      "        [1.2273702e-02, 1.2780285e-03, 1.1275173e-05, 9.8643702e-01],\n",
      "        [9.9999082e-01, 1.7399517e-09, 6.9546559e-06, 2.2388210e-06],\n",
      "        [5.0965111e-05, 5.6370236e-06, 9.9994338e-01, 2.0265144e-10]],\n",
      "\n",
      "       [[5.1600515e-09, 9.9999809e-01, 1.9177148e-06, 4.1833123e-10],\n",
      "        [1.1763201e-02, 1.2759370e-02, 9.5073581e-01, 2.4741584e-02],\n",
      "        [5.0692544e-03, 1.5182076e-09, 7.4778892e-02, 9.2015189e-01],\n",
      "        [9.9986994e-01, 2.2628792e-05, 2.4189130e-06, 1.0503438e-04]],\n",
      "\n",
      "       [[9.9999905e-01, 7.9517314e-08, 8.0059436e-07, 4.5256041e-08],\n",
      "        [1.5391886e-03, 2.5368556e-02, 9.6936506e-01, 3.7272172e-03],\n",
      "        [3.2177639e-05, 3.0155936e-01, 1.9225357e-01, 5.0615489e-01],\n",
      "        [9.1000913e-09, 5.7119154e-04, 4.5808082e-09, 9.9942881e-01]],\n",
      "\n",
      "       [[9.9851555e-01, 4.9390371e-07, 1.4839231e-03, 1.5480831e-08],\n",
      "        [8.2561506e-05, 6.3586567e-04, 9.9868959e-01, 5.9189525e-04],\n",
      "        [5.5442762e-04, 1.6621441e-01, 1.3774524e-02, 8.1945664e-01],\n",
      "        [3.0795354e-03, 9.9691260e-01, 5.2503140e-09, 7.8047069e-06]],\n",
      "\n",
      "       [[3.4584648e-09, 3.2706900e-08, 9.1843945e-01, 8.1560574e-02],\n",
      "        [1.9978037e-08, 1.2404497e-07, 2.9084641e-03, 9.9709141e-01],\n",
      "        [7.8962624e-01, 2.1003620e-01, 8.5435221e-07, 3.3668778e-04],\n",
      "        [3.2755085e-03, 9.9665666e-01, 6.7787863e-05, 4.9102382e-11]],\n",
      "\n",
      "       [[4.3258999e-05, 9.6693826e-01, 1.3175594e-07, 3.3018358e-02],\n",
      "        [4.5587108e-04, 3.9644292e-04, 4.0015093e-06, 9.9914372e-01],\n",
      "        [9.9995828e-01, 6.3186967e-09, 3.0482463e-05, 1.1208376e-05],\n",
      "        [1.7201810e-05, 3.5597441e-06, 9.9997926e-01, 2.5869276e-10]],\n",
      "\n",
      "       [[9.9999845e-01, 6.6849966e-09, 3.2258981e-08, 1.5182246e-06],\n",
      "        [3.7180397e-04, 6.0575511e-03, 2.7476302e-01, 7.1880758e-01],\n",
      "        [3.2753253e-08, 1.1320465e-02, 9.8767579e-01, 1.0037045e-03],\n",
      "        [3.0028479e-05, 9.9994767e-01, 2.0412443e-05, 1.9648219e-06]]],\n",
      "      dtype=float32)]\n",
      "[array([[[2.50969329e-10, 2.26095920e-09, 5.47825038e-01, 4.52174902e-01],\n",
      "        [1.13546803e-05, 1.64812755e-05, 9.96060431e-01, 3.91173130e-03],\n",
      "        [9.99789298e-01, 2.10698156e-04, 3.09179704e-08, 7.64806107e-10],\n",
      "        [8.35027531e-05, 9.99915838e-01, 5.57313399e-07, 2.95273445e-10]],\n",
      "\n",
      "       [[2.17505813e-09, 9.24762726e-01, 1.12317719e-06, 7.52361864e-02],\n",
      "        [6.31660910e-08, 3.82167673e-05, 2.53564522e-05, 9.99936342e-01],\n",
      "        [9.62606370e-02, 9.82871882e-08, 9.02357042e-01, 1.38219877e-03],\n",
      "        [9.87073064e-01, 1.92987636e-06, 1.29249915e-02, 2.00297947e-08]],\n",
      "\n",
      "       [[1.81646769e-07, 9.99660730e-01, 4.79958961e-09, 3.39012797e-04],\n",
      "        [1.69283259e-04, 1.54108304e-04, 3.71198257e-06, 9.99672890e-01],\n",
      "        [9.99924660e-01, 4.07945766e-09, 5.85089256e-05, 1.67816743e-05],\n",
      "        [2.44944713e-05, 3.81680184e-06, 9.99971747e-01, 4.01957162e-10]],\n",
      "\n",
      "       [[7.51910534e-08, 6.65315929e-06, 2.14574136e-09, 9.99993324e-01],\n",
      "        [2.29885522e-03, 9.97685790e-01, 9.25267796e-06, 6.00840212e-06],\n",
      "        [9.80236948e-01, 2.09379103e-03, 1.76693033e-02, 8.11028222e-09],\n",
      "        [1.52501514e-08, 4.27454350e-10, 9.99999881e-01, 6.96459495e-08]],\n",
      "\n",
      "       [[3.12731676e-02, 1.08336121e-06, 9.68597591e-01, 1.28215819e-04],\n",
      "        [8.37199211e-01, 2.76454412e-05, 1.19637534e-01, 4.31356169e-02],\n",
      "        [3.07931303e-04, 1.36840536e-05, 3.99633689e-04, 9.99278724e-01],\n",
      "        [5.65238297e-04, 9.93810952e-01, 1.10207266e-08, 5.62378811e-03]],\n",
      "\n",
      "       [[6.72012099e-12, 2.89251102e-06, 9.83919203e-01, 1.60778090e-02],\n",
      "        [1.52031193e-10, 7.78749527e-04, 4.25146148e-03, 9.94969785e-01],\n",
      "        [7.51544628e-03, 9.92438793e-01, 1.07885342e-07, 4.56296948e-05],\n",
      "        [9.99509215e-01, 4.83110518e-04, 7.66735047e-06, 1.62027100e-10]],\n",
      "\n",
      "       [[2.88972147e-02, 9.71076369e-01, 2.64079317e-05, 4.39345413e-08],\n",
      "        [9.91895974e-01, 7.05530168e-03, 1.04672753e-03, 2.03126478e-06],\n",
      "        [1.00194333e-08, 4.55858924e-08, 9.99881744e-01, 1.18265809e-04],\n",
      "        [1.53694074e-07, 1.32912170e-08, 2.52374703e-05, 9.99974608e-01]],\n",
      "\n",
      "       [[8.31519742e-09, 1.98427108e-10, 1.92503408e-02, 9.80749667e-01],\n",
      "        [1.54914567e-04, 2.92592649e-05, 9.94212329e-01, 5.60343126e-03],\n",
      "        [9.99884009e-01, 1.15999959e-04, 9.05211728e-09, 8.73798187e-11],\n",
      "        [1.28402113e-04, 9.99869943e-01, 1.61183539e-06, 1.57368718e-09]],\n",
      "\n",
      "       [[2.55056866e-03, 5.31837641e-09, 2.71652170e-05, 9.97422218e-01],\n",
      "        [9.99273121e-01, 5.37584901e-06, 5.95367339e-04, 1.26167521e-04],\n",
      "        [9.01341082e-06, 7.82468996e-05, 9.99912739e-01, 2.10781810e-08],\n",
      "        [1.27304811e-04, 9.99864817e-01, 7.82772986e-06, 8.79567885e-09]],\n",
      "\n",
      "       [[3.04025694e-11, 1.20748346e-05, 9.87651944e-01, 1.23358797e-02],\n",
      "        [1.40833323e-09, 3.58975399e-03, 1.09778028e-02, 9.85432446e-01],\n",
      "        [1.41199154e-03, 9.98532414e-01, 2.67385360e-07, 5.53991013e-05],\n",
      "        [9.99591291e-01, 3.98661447e-04, 9.98064661e-06, 3.43529954e-10]],\n",
      "\n",
      "       [[4.31478202e-01, 3.23883396e-05, 5.68487346e-01, 2.11478050e-06],\n",
      "        [1.19381445e-03, 3.69933015e-03, 9.91918147e-01, 3.18871788e-03],\n",
      "        [7.79265363e-04, 9.62104678e-01, 6.28252281e-04, 3.64877731e-02],\n",
      "        [2.26757311e-06, 1.59961835e-03, 4.89206364e-09, 9.98398125e-01]],\n",
      "\n",
      "       [[7.25911319e-01, 1.24112994e-05, 1.43611487e-06, 2.74074823e-01],\n",
      "        [1.93713531e-05, 7.17098825e-03, 2.57302901e-07, 9.92809415e-01],\n",
      "        [5.04594063e-04, 9.95400608e-01, 4.05285181e-03, 4.20201213e-05],\n",
      "        [5.11198277e-06, 1.64801975e-07, 9.99994755e-01, 2.67101297e-09]],\n",
      "\n",
      "       [[1.44232307e-10, 3.78291105e-07, 9.99999642e-01, 6.46218590e-09],\n",
      "        [2.22028862e-03, 9.95888412e-01, 9.52917326e-04, 9.38399637e-04],\n",
      "        [9.84058082e-01, 1.29792634e-02, 5.31732169e-08, 2.96264468e-03],\n",
      "        [2.35468669e-05, 2.46932684e-03, 1.28756747e-05, 9.97494221e-01]],\n",
      "\n",
      "       [[9.93086517e-01, 5.64748916e-06, 6.90723490e-03, 5.77970354e-07],\n",
      "        [4.09348024e-04, 5.76814264e-03, 9.92652297e-01, 1.17016339e-03],\n",
      "        [2.89532822e-04, 9.53538716e-01, 2.04900815e-03, 4.41227295e-02],\n",
      "        [6.26032829e-07, 1.03447901e-03, 4.24007629e-09, 9.98964906e-01]],\n",
      "\n",
      "       [[2.38092369e-07, 9.88389611e-01, 2.76886762e-08, 1.16100898e-02],\n",
      "        [3.54693148e-05, 1.95599132e-04, 5.67558254e-06, 9.99763310e-01],\n",
      "        [9.99403358e-01, 1.46149688e-08, 5.18074783e-04, 7.85134398e-05],\n",
      "        [2.53992803e-05, 2.36086157e-06, 9.99972224e-01, 5.85324345e-10]],\n",
      "\n",
      "       [[9.99308228e-01, 1.46351624e-07, 1.42305261e-08, 6.91636000e-04],\n",
      "        [3.80743574e-03, 7.78566971e-02, 9.39586243e-07, 9.18334901e-01],\n",
      "        [3.80106911e-04, 9.97749150e-01, 1.85217697e-03, 1.85059580e-05],\n",
      "        [3.12634643e-06, 2.87600130e-07, 9.99996543e-01, 4.35431025e-09]]],\n",
      "      dtype=float32)]\n",
      "[array([[[5.23024946e-02, 9.95186383e-07, 9.47696567e-01, 5.92604321e-09],\n",
      "        [9.99989152e-01, 1.56060196e-06, 8.72769670e-06, 6.46539149e-07],\n",
      "        [1.04341167e-03, 9.86995399e-01, 1.50548403e-05, 1.19461874e-02],\n",
      "        [3.62836494e-09, 7.58208008e-03, 4.04519085e-07, 9.92417455e-01]],\n",
      "\n",
      "       [[3.28832829e-11, 9.94691789e-01, 3.12673365e-04, 4.99552488e-03],\n",
      "        [6.18625096e-10, 1.38661762e-05, 4.71655047e-03, 9.95269597e-01],\n",
      "        [7.98866211e-04, 2.63094165e-07, 9.98975396e-01, 2.25504817e-04],\n",
      "        [9.99976158e-01, 2.27693126e-05, 1.12231919e-06, 1.21258150e-08]],\n",
      "\n",
      "       [[5.30787073e-02, 9.46920872e-01, 4.37831005e-07, 3.54607437e-08],\n",
      "        [9.97935176e-01, 2.04665656e-03, 1.77607453e-05, 5.21049913e-07],\n",
      "        [2.97983643e-05, 2.51715446e-05, 9.95573044e-01, 4.37199650e-03],\n",
      "        [2.16018448e-09, 2.40015341e-09, 5.67218568e-03, 9.94327843e-01]],\n",
      "\n",
      "       [[1.47242652e-11, 2.41982634e-04, 9.99758065e-01, 1.74914287e-08],\n",
      "        [7.40277465e-05, 9.99420047e-01, 3.41882900e-04, 1.64021418e-04],\n",
      "        [6.33322746e-02, 2.89853495e-02, 5.25342443e-07, 9.07681882e-01],\n",
      "        [9.99960780e-01, 3.77586948e-05, 1.58010607e-07, 1.25241831e-06]],\n",
      "\n",
      "       [[5.36997011e-03, 4.33634284e-07, 9.94629622e-01, 2.06542672e-09],\n",
      "        [9.99990344e-01, 2.64159235e-06, 6.32458841e-06, 7.48156310e-07],\n",
      "        [5.28544246e-04, 9.96978045e-01, 1.93754568e-06, 2.49153236e-03],\n",
      "        [5.24534327e-09, 2.58159032e-03, 9.33147817e-07, 9.97417450e-01]],\n",
      "\n",
      "       [[4.51915897e-02, 9.61852720e-07, 9.54807460e-01, 2.67522449e-08],\n",
      "        [9.99952316e-01, 1.78341168e-06, 4.29045431e-05, 3.02536773e-06],\n",
      "        [5.24435379e-03, 5.45948923e-01, 5.91911608e-04, 4.48214859e-01],\n",
      "        [2.87472940e-05, 9.99943256e-01, 2.00475725e-07, 2.77886102e-05]],\n",
      "\n",
      "       [[7.21418161e-08, 9.99999642e-01, 3.26339240e-08, 2.71496532e-07],\n",
      "        [2.81040490e-01, 3.49634588e-02, 2.76782666e-03, 6.81228280e-01],\n",
      "        [4.62829469e-10, 7.01054770e-11, 1.30626280e-03, 9.98693764e-01],\n",
      "        [8.41474626e-03, 7.26665239e-05, 9.90951657e-01, 5.60832734e-04]],\n",
      "\n",
      "       [[9.98208284e-01, 2.26109992e-10, 1.36498923e-09, 1.79170386e-03],\n",
      "        [2.70854219e-08, 3.36403843e-07, 6.58971039e-05, 9.99933720e-01],\n",
      "        [1.49748480e-08, 5.15988749e-03, 9.94688272e-01, 1.51835658e-04],\n",
      "        [1.59961419e-05, 9.97830212e-01, 2.14835326e-03, 5.49352671e-06]],\n",
      "\n",
      "       [[5.56058949e-11, 7.40285218e-02, 9.25971448e-01, 4.24278657e-09],\n",
      "        [1.54559326e-04, 9.89568532e-01, 1.01768719e-02, 1.00008991e-04],\n",
      "        [2.99864441e-01, 3.87631096e-02, 6.33098944e-06, 6.61366105e-01],\n",
      "        [4.48386345e-07, 6.76039272e-05, 6.02340378e-06, 9.99925971e-01]],\n",
      "\n",
      "       [[1.23917867e-04, 9.99872208e-01, 2.28045902e-08, 3.76867638e-06],\n",
      "        [9.99859333e-01, 1.01653779e-04, 2.34407139e-07, 3.86994325e-05],\n",
      "        [3.33213793e-05, 1.02281241e-07, 1.08208551e-04, 9.99858379e-01],\n",
      "        [2.14149547e-03, 3.69954904e-07, 9.97722685e-01, 1.35446302e-04]],\n",
      "\n",
      "       [[7.16876593e-06, 9.99992847e-01, 5.32380096e-09, 4.53654323e-08],\n",
      "        [9.99741375e-01, 2.26717922e-04, 2.17381398e-06, 2.97666702e-05],\n",
      "        [1.39330932e-05, 5.23365429e-07, 1.29673816e-03, 9.98688757e-01],\n",
      "        [2.87324394e-04, 2.89071409e-07, 9.99665260e-01, 4.72210631e-05]],\n",
      "\n",
      "       [[9.87544417e-01, 1.02365110e-02, 1.19253434e-06, 2.21798313e-03],\n",
      "        [1.66020857e-03, 9.88793790e-01, 1.60561649e-06, 9.54438280e-03],\n",
      "        [5.23712924e-06, 2.25498457e-06, 8.43603193e-05, 9.99908090e-01],\n",
      "        [9.45042470e-04, 4.35222660e-07, 9.98878539e-01, 1.75997440e-04]],\n",
      "\n",
      "       [[9.99998093e-01, 1.83333825e-07, 1.61497644e-06, 2.41561715e-10],\n",
      "        [9.99172800e-04, 1.01332285e-01, 8.97518933e-01, 1.49620071e-04],\n",
      "        [6.88004584e-05, 9.96699035e-01, 2.46460317e-03, 7.67602993e-04],\n",
      "        [1.31396234e-07, 2.34897816e-04, 2.18324541e-08, 9.99764979e-01]],\n",
      "\n",
      "       [[3.44429054e-07, 3.92621189e-08, 9.99999523e-01, 1.00760069e-07],\n",
      "        [9.90749061e-01, 3.51999952e-05, 2.28109909e-03, 6.93461997e-03],\n",
      "        [1.39426615e-03, 2.88187742e-01, 2.82267520e-05, 7.10389793e-01],\n",
      "        [3.51555864e-05, 9.99785244e-01, 3.83588230e-07, 1.79247407e-04]],\n",
      "\n",
      "       [[2.15550499e-05, 6.10712974e-08, 9.99978304e-01, 7.59915508e-08],\n",
      "        [9.98899221e-01, 3.92694847e-06, 4.75631648e-04, 6.21204032e-04],\n",
      "        [6.15779404e-03, 1.01706171e-02, 3.87965119e-05, 9.83632743e-01],\n",
      "        [8.73437166e-05, 9.99442995e-01, 4.54521825e-08, 4.69672552e-04]],\n",
      "\n",
      "       [[8.85459137e-11, 3.67022294e-05, 2.30037404e-06, 9.99961019e-01],\n",
      "        [4.01556099e-06, 9.90772724e-01, 9.03383363e-03, 1.89479033e-04],\n",
      "        [9.89713135e-06, 4.00015095e-04, 9.99586403e-01, 3.64385505e-06],\n",
      "        [9.98462796e-01, 3.44951984e-10, 1.53722614e-03, 7.46257556e-10]]],\n",
      "      dtype=float32)]\n",
      "[array([[[2.94901259e-08, 1.84859819e-04, 1.21452204e-09, 9.99815166e-01],\n",
      "        [7.99951842e-04, 9.99178708e-01, 1.67909184e-05, 4.50900325e-06],\n",
      "        [9.78460848e-01, 2.12827581e-03, 1.94108766e-02, 1.95144665e-08],\n",
      "        [3.80512660e-07, 1.11039522e-09, 9.99999523e-01, 1.12516446e-07]],\n",
      "\n",
      "       [[8.54197983e-03, 3.58527094e-01, 6.35111121e-07, 6.32930279e-01],\n",
      "        [2.57489026e-01, 7.42506802e-01, 2.26700436e-07, 3.92966604e-06],\n",
      "        [9.99951601e-01, 2.62670765e-05, 2.22220442e-05, 1.79927982e-11],\n",
      "        [4.99745729e-05, 2.97352862e-08, 9.99949694e-01, 3.90466738e-07]],\n",
      "\n",
      "       [[1.02251645e-11, 1.18521442e-08, 3.05379212e-01, 6.94620788e-01],\n",
      "        [1.25351875e-07, 2.05275559e-04, 9.46463466e-01, 5.33311814e-02],\n",
      "        [4.41005500e-03, 9.95580971e-01, 9.16134297e-07, 8.10615438e-06],\n",
      "        [9.99335229e-01, 6.64366584e-04, 3.93214634e-07, 6.24069615e-11]],\n",
      "\n",
      "       [[1.44232778e-10, 3.03837508e-01, 6.96162522e-01, 1.13279048e-08],\n",
      "        [2.61444424e-04, 9.31733370e-01, 6.78040981e-02, 2.00998897e-04],\n",
      "        [1.14809163e-01, 3.15750646e-03, 2.65512026e-05, 8.82006824e-01],\n",
      "        [9.99921203e-01, 7.76931556e-05, 5.80870407e-10, 1.03401305e-06]],\n",
      "\n",
      "       [[1.84040327e-09, 2.03264668e-03, 2.60348372e-07, 9.97967064e-01],\n",
      "        [1.26023544e-04, 9.99609292e-01, 2.27905548e-04, 3.66862041e-05],\n",
      "        [2.98007997e-03, 2.01807264e-03, 9.94996071e-01, 5.80063079e-06],\n",
      "        [9.83787537e-01, 1.35982656e-10, 1.62124895e-02, 7.59702690e-10]],\n",
      "\n",
      "       [[2.78509349e-01, 1.25344217e-04, 7.07045078e-01, 1.43201891e-02],\n",
      "        [3.40742463e-06, 1.02424739e-04, 9.37392354e-01, 6.25018477e-02],\n",
      "        [9.43019813e-08, 5.56100895e-05, 1.48452324e-04, 9.99795854e-01],\n",
      "        [2.42445841e-02, 9.75750864e-01, 1.59422164e-09, 4.57389478e-06]],\n",
      "\n",
      "       [[9.98193920e-01, 1.80568220e-03, 8.57135944e-08, 2.22475563e-07],\n",
      "        [7.65990815e-04, 9.99161005e-01, 6.81334059e-05, 4.83446775e-06],\n",
      "        [2.83863501e-05, 5.94684109e-03, 9.72923219e-01, 2.11015213e-02],\n",
      "        [2.16422897e-08, 8.37898240e-12, 2.20984127e-03, 9.97790098e-01]],\n",
      "\n",
      "       [[5.29837216e-11, 6.04930276e-04, 1.55237703e-05, 9.99379516e-01],\n",
      "        [1.08489280e-06, 9.36211646e-01, 6.34364262e-02, 3.50724760e-04],\n",
      "        [1.29576779e-06, 8.26004270e-06, 9.99989271e-01, 1.22073243e-06],\n",
      "        [9.99956012e-01, 9.36836031e-09, 4.40421172e-05, 7.49792339e-10]],\n",
      "\n",
      "       [[8.50655317e-01, 1.96920678e-06, 1.49342775e-01, 1.20715136e-08],\n",
      "        [7.70546822e-03, 4.30893619e-03, 9.84661400e-01, 3.32421623e-03],\n",
      "        [6.62147626e-03, 8.70073736e-01, 1.10568723e-03, 1.22199170e-01],\n",
      "        [5.00329008e-07, 3.05275642e-03, 3.03183434e-09, 9.96946752e-01]],\n",
      "\n",
      "       [[9.11739093e-08, 9.31104496e-02, 9.06889498e-01, 2.55047161e-09],\n",
      "        [7.62692699e-03, 9.70165193e-01, 2.21542176e-02, 5.37029846e-05],\n",
      "        [9.94706571e-01, 4.28946316e-03, 7.10997767e-07, 1.00325746e-03],\n",
      "        [1.13961846e-06, 8.89352872e-04, 1.95141147e-05, 9.99089956e-01]],\n",
      "\n",
      "       [[3.17873128e-06, 9.99992013e-01, 4.77796084e-06, 2.67259437e-10],\n",
      "        [9.06278014e-01, 6.08064560e-03, 8.75216201e-02, 1.19698750e-04],\n",
      "        [1.87577655e-07, 3.86399392e-08, 9.97171462e-01, 2.82833958e-03],\n",
      "        [2.15127307e-06, 1.62532068e-08, 2.47284811e-06, 9.99995351e-01]],\n",
      "\n",
      "       [[2.28206357e-10, 6.97631061e-01, 3.02368969e-01, 5.81743653e-10],\n",
      "        [7.77376466e-04, 5.31283766e-02, 9.41150188e-01, 4.94402554e-03],\n",
      "        [9.11496818e-01, 1.32886399e-07, 7.26418570e-04, 8.77766982e-02],\n",
      "        [1.68147540e-06, 1.89539787e-05, 3.97134450e-08, 9.99979377e-01]],\n",
      "\n",
      "       [[8.02048705e-12, 1.99862257e-07, 1.73316442e-03, 9.98266578e-01],\n",
      "        [3.87624866e-09, 8.40585306e-03, 9.89899218e-01, 1.69493642e-03],\n",
      "        [3.55168086e-05, 9.99925494e-01, 3.37443453e-05, 5.27751172e-06],\n",
      "        [9.99777615e-01, 2.22346585e-04, 6.88626711e-09, 2.19090371e-11]],\n",
      "\n",
      "       [[7.05160928e-05, 9.99855876e-01, 1.73392056e-08, 7.35916256e-05],\n",
      "        [9.75793600e-01, 1.08041242e-03, 3.39534927e-06, 2.31225323e-02],\n",
      "        [1.14365184e-05, 1.36596494e-08, 3.96511787e-05, 9.99948859e-01],\n",
      "        [6.79776305e-03, 5.99051418e-07, 9.92930412e-01, 2.71141704e-04]],\n",
      "\n",
      "       [[4.33426350e-11, 1.79194346e-01, 8.20805609e-01, 4.81385543e-10],\n",
      "        [5.26386662e-04, 9.68564928e-01, 3.07838265e-02, 1.24841972e-04],\n",
      "        [8.79938900e-01, 6.84380112e-03, 2.19332469e-06, 1.13215081e-01],\n",
      "        [3.52580685e-07, 7.63434509e-05, 6.15247473e-06, 9.99917150e-01]],\n",
      "\n",
      "       [[1.38465020e-06, 9.99992132e-01, 6.48262721e-06, 1.81697261e-11],\n",
      "        [9.25406456e-01, 3.88851343e-03, 7.06389621e-02, 6.59385623e-05],\n",
      "        [2.62479580e-06, 1.52086983e-07, 9.90210414e-01, 9.78676789e-03],\n",
      "        [8.60007549e-06, 3.58914960e-08, 1.30227522e-06, 9.99990106e-01]]],\n",
      "      dtype=float32)]\n",
      "[array([[[7.0455317e-06, 9.9998784e-01, 5.0820190e-06, 4.4074942e-11],\n",
      "        [9.9011922e-01, 1.1040538e-03, 8.7665590e-03, 1.0159153e-05],\n",
      "        [1.4856178e-06, 6.7264295e-08, 9.9699545e-01, 3.0030590e-03],\n",
      "        [2.2743729e-06, 1.8238259e-08, 3.6723379e-06, 9.9999404e-01]],\n",
      "\n",
      "       [[2.0157064e-11, 1.5609902e-03, 3.0054894e-01, 6.9789010e-01],\n",
      "        [6.3048216e-09, 8.0899191e-01, 1.8953815e-01, 1.4699349e-03],\n",
      "        [7.9209823e-04, 9.9578899e-01, 3.2773684e-03, 1.4146023e-04],\n",
      "        [9.9999952e-01, 4.6027941e-07, 2.7008209e-09, 6.2552962e-12]],\n",
      "\n",
      "       [[4.8634548e-11, 1.0633846e-07, 9.9955946e-01, 4.4034838e-04],\n",
      "        [8.9816737e-10, 2.7723399e-05, 5.4530002e-04, 9.9942696e-01],\n",
      "        [3.8620293e-02, 9.6135855e-01, 2.1322970e-08, 2.1219304e-05],\n",
      "        [9.9877495e-01, 1.2148848e-03, 1.0155423e-05, 1.0397138e-10]],\n",
      "\n",
      "       [[9.9999940e-01, 2.0422142e-07, 3.8871349e-07, 1.4688133e-10],\n",
      "        [1.5245519e-03, 5.2807814e-01, 4.7032091e-01, 7.6418874e-05],\n",
      "        [8.2749320e-05, 9.9545431e-01, 4.3513007e-03, 1.1165880e-04],\n",
      "        [5.5347986e-08, 1.0888920e-04, 1.3392865e-07, 9.9989092e-01]],\n",
      "\n",
      "       [[9.1207172e-12, 2.3413571e-05, 9.9997568e-01, 9.9182944e-07],\n",
      "        [7.9609981e-06, 9.9082142e-01, 9.8671287e-04, 8.1839021e-03],\n",
      "        [2.2933004e-06, 2.1613431e-04, 1.4263278e-08, 9.9978155e-01],\n",
      "        [9.9981302e-01, 1.8439686e-04, 2.2508333e-07, 2.3694779e-06]],\n",
      "\n",
      "       [[9.9997270e-01, 2.7207632e-05, 7.1417560e-08, 3.1168110e-10],\n",
      "        [3.2587559e-05, 9.9862683e-01, 1.3404757e-03, 1.2906541e-07],\n",
      "        [8.6552791e-06, 7.1019703e-03, 9.9060482e-01, 2.2846027e-03],\n",
      "        [8.5899137e-06, 8.0662477e-11, 4.9146893e-06, 9.9998653e-01]],\n",
      "\n",
      "       [[4.1639150e-06, 2.2542400e-10, 2.4242961e-04, 9.9975342e-01],\n",
      "        [2.5791474e-02, 4.8582348e-05, 9.6528763e-01, 8.8723367e-03],\n",
      "        [9.9997306e-01, 2.6803304e-05, 1.1947574e-07, 5.8470638e-11],\n",
      "        [6.7260902e-07, 9.9999928e-01, 2.9592028e-08, 1.2371471e-11]],\n",
      "\n",
      "       [[9.9220157e-01, 2.0232123e-10, 1.2938443e-09, 7.7983616e-03],\n",
      "        [3.9512095e-08, 2.2813880e-07, 2.7820191e-05, 9.9997187e-01],\n",
      "        [4.9845838e-08, 2.7697923e-02, 9.7194552e-01, 3.5650152e-04],\n",
      "        [1.6896174e-05, 9.9354833e-01, 6.4262128e-03, 8.5349684e-06]],\n",
      "\n",
      "       [[2.0941285e-07, 9.9999952e-01, 2.5051679e-07, 2.8690663e-12],\n",
      "        [9.9175125e-01, 7.9421885e-04, 7.4440842e-03, 1.0361314e-05],\n",
      "        [2.3374105e-06, 5.8316264e-08, 9.9607480e-01, 3.9228518e-03],\n",
      "        [1.4123085e-06, 1.7855992e-08, 5.7301795e-06, 9.9999285e-01]],\n",
      "\n",
      "       [[1.6057178e-01, 8.3939886e-01, 2.9437384e-05, 5.0481419e-09],\n",
      "        [9.7197008e-01, 2.3691118e-02, 4.3377108e-03, 1.0361376e-06],\n",
      "        [7.0455586e-10, 1.5384460e-08, 9.9997556e-01, 2.4449568e-05],\n",
      "        [5.9797219e-07, 2.4669635e-08, 1.4699247e-05, 9.9998462e-01]],\n",
      "\n",
      "       [[1.0684149e-06, 3.0897233e-11, 1.1322414e-05, 9.9998760e-01],\n",
      "        [1.8509033e-01, 3.8707363e-05, 8.0534714e-01, 9.5237587e-03],\n",
      "        [9.9995410e-01, 4.5182922e-05, 6.5642087e-07, 1.4534028e-10],\n",
      "        [2.2791075e-07, 9.9999976e-01, 2.6664770e-08, 9.4936489e-12]],\n",
      "\n",
      "       [[4.3622962e-01, 1.2593502e-07, 7.6044386e-04, 5.6300986e-01],\n",
      "        [9.8184913e-01, 1.6638818e-05, 1.6759494e-02, 1.3747136e-03],\n",
      "        [2.2320517e-05, 8.5227057e-06, 9.9996901e-01, 1.5835346e-07],\n",
      "        [3.0166705e-04, 9.9967134e-01, 2.6336811e-05, 6.8595358e-07]],\n",
      "\n",
      "       [[9.7418971e-09, 1.0925775e-12, 1.0820178e-06, 9.9999893e-01],\n",
      "        [1.8935861e-02, 7.5760991e-06, 9.7578841e-01, 5.2681915e-03],\n",
      "        [9.9962962e-01, 3.4646058e-04, 2.3940685e-05, 2.0275999e-09],\n",
      "        [1.0831962e-07, 9.9999988e-01, 2.8091403e-08, 1.3246205e-11]],\n",
      "\n",
      "       [[9.9996710e-01, 1.1175333e-07, 2.8339231e-05, 4.3693881e-06],\n",
      "        [9.2554598e-07, 1.2916316e-04, 9.3486875e-01, 6.5001249e-02],\n",
      "        [4.8615112e-09, 1.6820551e-06, 1.2697951e-03, 9.9872845e-01],\n",
      "        [1.5383480e-03, 9.9845040e-01, 1.3285875e-08, 1.1147779e-05]],\n",
      "\n",
      "       [[3.3525486e-02, 5.3847571e-10, 4.5249317e-07, 9.6647406e-01],\n",
      "        [9.9500078e-01, 1.4336715e-06, 4.4495366e-03, 5.4825912e-04],\n",
      "        [3.0132995e-07, 1.2856713e-06, 9.9999845e-01, 2.4579565e-08],\n",
      "        [7.4427418e-04, 9.9905068e-01, 2.0021822e-04, 4.9156561e-06]],\n",
      "\n",
      "       [[5.4199902e-09, 1.0433934e-06, 9.9999893e-01, 2.3419236e-10],\n",
      "        [2.0902656e-02, 9.7859573e-01, 3.3347361e-04, 1.6808053e-04],\n",
      "        [9.8233217e-01, 1.7576614e-02, 5.6799063e-08, 9.1224363e-05],\n",
      "        [2.0603607e-05, 9.8795705e-03, 5.7306632e-05, 9.9004245e-01]]],\n",
      "      dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    " #now how does it work?\n",
    "test = gen_data(time_steps=4, batch_size=16, num_batches=40)\n",
    "for j in range(5):\n",
    "    X,Y = test.next()\n",
    "    preds = session.run([predictions], feed_dict={x:X, y:Y})\n",
    "    print preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"stack_6:0\", shape=(16, 4, 1), dtype=int64)\n",
      "[[2.37803393e-01 3.26576636e-01 8.45999434e-01 1.42052693e-01]\n",
      " [7.45903598e-02 4.63038922e-01 1.87121570e-01 7.74748501e-01]\n",
      " [9.26539843e-01 4.89085009e-02 9.67016609e-01 9.73806360e-01]\n",
      " [2.30957750e-01 7.85099418e-01 9.48474318e-01 2.38522393e-01]\n",
      " [7.44165164e-01 5.35791462e-01 4.85957890e-02 5.19951853e-01]\n",
      " [8.87483776e-01 7.75138081e-01 8.54927568e-01 3.59461462e-01]\n",
      " [4.98390064e-01 7.42661295e-01 2.67875810e-02 7.57954040e-01]\n",
      " [5.22705850e-01 3.29713964e-01 1.75705277e-01 7.50263723e-02]\n",
      " [4.99074725e-01 2.01264448e-02 1.62929486e-02 4.24982745e-01]\n",
      " [3.66449647e-01 6.53045660e-01 5.67694131e-02 2.54042758e-04]\n",
      " [5.09443616e-01 6.41867075e-01 2.82470030e-01 9.48296808e-01]\n",
      " [6.50622567e-01 6.28774732e-01 8.74792172e-01 9.31606169e-02]\n",
      " [7.54022311e-01 5.59116377e-01 7.94327680e-01 5.76325657e-01]\n",
      " [2.59094492e-01 9.90491930e-01 9.26564356e-01 9.76869597e-01]\n",
      " [1.84630591e-01 6.03126025e-01 9.92510302e-01 9.04214565e-01]\n",
      " [9.79406664e-01 9.88258042e-01 5.82690146e-01 8.68152504e-01]]\n",
      "[[3 0 1 2]\n",
      " [0 2 1 3]\n",
      " [1 0 2 3]\n",
      " [0 3 1 2]\n",
      " [2 3 1 0]\n",
      " [3 1 2 0]\n",
      " [2 0 1 3]\n",
      " [3 2 1 0]\n",
      " [2 1 3 0]\n",
      " [3 2 0 1]\n",
      " [2 0 1 3]\n",
      " [3 1 0 2]\n",
      " [1 3 0 2]\n",
      " [0 2 3 1]\n",
      " [0 1 3 2]\n",
      " [2 3 0 1]]\n",
      "[array([[[3],\n",
      "        [0],\n",
      "        [1],\n",
      "        [2]],\n",
      "\n",
      "       [[0],\n",
      "        [2],\n",
      "        [1],\n",
      "        [3]],\n",
      "\n",
      "       [[1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [0]],\n",
      "\n",
      "       [[3],\n",
      "        [0],\n",
      "        [1],\n",
      "        [2]],\n",
      "\n",
      "       [[2],\n",
      "        [3],\n",
      "        [1],\n",
      "        [0]],\n",
      "\n",
      "       [[3],\n",
      "        [1],\n",
      "        [2],\n",
      "        [0]],\n",
      "\n",
      "       [[2],\n",
      "        [0],\n",
      "        [1],\n",
      "        [3]],\n",
      "\n",
      "       [[3],\n",
      "        [2],\n",
      "        [1],\n",
      "        [0]],\n",
      "\n",
      "       [[2],\n",
      "        [1],\n",
      "        [3],\n",
      "        [0]],\n",
      "\n",
      "       [[3],\n",
      "        [2],\n",
      "        [0],\n",
      "        [1]],\n",
      "\n",
      "       [[2],\n",
      "        [0],\n",
      "        [1],\n",
      "        [3]],\n",
      "\n",
      "       [[3],\n",
      "        [1],\n",
      "        [0],\n",
      "        [2]],\n",
      "\n",
      "       [[1],\n",
      "        [3],\n",
      "        [0],\n",
      "        [2]],\n",
      "\n",
      "       [[0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1]],\n",
      "\n",
      "       [[0],\n",
      "        [1],\n",
      "        [3],\n",
      "        [2]],\n",
      "\n",
      "       [[2],\n",
      "        [3],\n",
      "        [1],\n",
      "        [0]]])]\n"
     ]
    }
   ],
   "source": [
    "#decoder cell\n",
    "with tf.variable_scope('decoder_cell', reuse=True):\n",
    "    W = tf.get_variable('W', [1 + state_size, state_size])\n",
    "    b = tf.get_variable('b', [state_size])\n",
    "\n",
    "state = final_state #start with state from encoder rnn\n",
    "\n",
    "decoder_input = tf.ones([batch_size, 1])\n",
    "decoder_input = tf.cast(tf.scalar_mul(-1, decoder_input), dtype=tf.float32)\n",
    "\n",
    "with tf.variable_scope('softmax', reuse=True):\n",
    "    s_W = tf.get_variable('W', [state_size, num_classes])\n",
    "    s_b = tf.get_variable('b', [num_classes])\n",
    "\n",
    "    \n",
    "i_predictions=[]\n",
    "\n",
    "for i in range(num_steps):\n",
    "    decoder_input = tf.cast(decoder_input, dtype=tf.float32)\n",
    "    state = rnn_cell(decoder_input, state, W, b)\n",
    "    i_logits=tf.matmul(state, s_W) + s_b\n",
    "    i_prediction=tf.nn.softmax(i_logits)\n",
    "    i_prediction = tf.argmax(i_prediction, axis=1)\n",
    "    i_prediction = tf.expand_dims(i_prediction, 1)\n",
    "    i_predictions.append(i_prediction)\n",
    "    decoder_input = i_prediction\n",
    "\n",
    "i_predictions = tf.stack(i_predictions, axis=1)\n",
    "print i_predictions\n",
    "\n",
    "test = gen_data(time_steps=4, batch_size=16, num_batches=40)\n",
    "\n",
    "X,Y = test.next()\n",
    "xxx = session.run([i_predictions], feed_dict={x:X, y:Y})\n",
    "print X\n",
    "print Y\n",
    "print xxx\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
